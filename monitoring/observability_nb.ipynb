{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a7065d-0bd0-4ca3-8d08-06b0b84cf19d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Implementing Azure Monitor Observability for the VA Data Engineering platform.\n",
    "\n",
    "* 1. Define required OTLP and Logs Ingestion API packages\n",
    "    * azure-core==1.30.0\n",
    "    * azure-identity==1.15.0\n",
    "    * azure-monitor-opentelemetry-exporter==1.0.0b22\n",
    "    * azure-monitor-ingestion==1.0.3\n",
    "    * azure-monitor-opentelemetry==1.0.0\n",
    "    * opentelemetry-api==1.22.0\n",
    "    * opentelemetry-sdk==1.22.0\n",
    "* 2. Use the Logs Ingestion API to track and send custom audit metrics/values to Log Analytics custom tables and OpenTelementry Protocol (OTLP) framework to export exception logs to the AppExceptions native table in Azure Monitor Log Analytics and the Exceptions Log table in Azure Monitor AppInsights.\n",
    "\n",
    "* 3. To edit the log analytics DCR and custom logs/tables schema requires two steps:\n",
    "    * Manually edit the schema of the custom log in the Log analytics tables view in the azure portal.\n",
    "    * Run the custom powershell code provided in the following link to edit the DCR: https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-rule-edit#putting-everything-together\n",
    "\n",
    "* 4. Info logs get exported to the traces logs. Exceptions get exported to the Exceptions log in App Insight and AppExceptions log in the Log Analytics workspace. Custom audits get sent to the specified custom log/table in the Log Analytics workspace using the logs ingestion API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad13494-de7e-4557-8c82-928e3af2747d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://pypi.org/project/azure-monitor-opentelemetry-exporter/\n",
    "# https://github.com/open-telemetry/opentelemetry-python\n",
    "# https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/monitor/azure-monitor-opentelemetry-exporter/samples/logs/sample_exception.py\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-configuration?tabs=python#set-the-cloud-role-name-and-the-cloud-role-instance\n",
    "# You can also set environment variables using the spark_env_vars field in the Create cluster request or Edit cluster request Clusters API endpoints. \n",
    "# https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters#--request-structure-of-the-cluster-definition\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab1ba291-bf09-43a6-b36d-49f5a77a5a95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Map Storage Account to Service Principal Object for Token Access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4b1d8e-a364-4cbd-8e4c-1e34ba4a70c7",
     "showTitle": true,
     "title": "Configure ADLS Gen 2 Access"
    }
   },
   "outputs": [],
   "source": [
    "# Permission is based on File or folder based ACL assignments to the Data Lake filesystem (container) . RBAC assignments to the top level Azure Data Lake resource is not required.\n",
    "# https://docs.databricks.com/storage/azure-storage.html\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adls05.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adls05.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adls05.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientid\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adls05.dfs.core.windows.net\", dbutils.secrets.get(\"myscope\", key=\"clientsecret\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adls05.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(dbutils.secrets.get(\"myscope\", key=\"tenantid\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4567e158-6094-4211-b091-2ebef2670e12",
     "showTitle": true,
     "title": "Import Required Packages"
    }
   },
   "outputs": [],
   "source": [
    "# https://realpython.com/python-logging/\n",
    "# https://opentelemetry.io/docs/instrumentation/python/\n",
    "# https://betterstack.com/community/guides/logging/how-to-start-logging-with-python/\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import DataFrame\n",
    "import azure.identity\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.ingestion import LogsIngestionClient\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from opentelemetry._logs import (\n",
    "    get_logger_provider,\n",
    "    set_logger_provider,\n",
    ")\n",
    "from opentelemetry.sdk._logs import (\n",
    "    LoggerProvider,\n",
    "    LoggingHandler,\n",
    ")\n",
    "from opentelemetry.sdk._logs.export import BatchLogRecordProcessor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorLogExporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d429b29b-e9f1-469c-b2a1-f8ba8f414e49",
     "showTitle": true,
     "title": "Set Client Secret Credential"
    }
   },
   "outputs": [],
   "source": [
    "tenant_id=dbutils.secrets.get(scope=\"myscope\", key=\"tenantid\")\n",
    "client_id = dbutils.secrets.get(scope=\"myscope\", key=\"clientid\")\n",
    "client_secret = dbutils.secrets.get(scope=\"myscope\", key=\"clientsecret\")\n",
    "#credential = DefaultAzureCredential()\n",
    "credential = azure.identity.ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "241b37ba-48ee-4ec8-a3c3-16e1fc204dc2",
     "showTitle": true,
     "title": "Configure OTLP Logger Components"
    }
   },
   "outputs": [],
   "source": [
    "set_logger_provider(LoggerProvider())\n",
    "exporter = AzureMonitorLogExporter.from_connection_string(\n",
    "    dbutils.secrets.get(\"myscope\", key=\"appinsightsconnstr\"), credential=credential\n",
    ")\n",
    "get_logger_provider().add_log_record_processor(BatchLogRecordProcessor(exporter))\n",
    "\n",
    "# Attach LoggingHandler to namespaced logger\n",
    "handler = LoggingHandler()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "511b7b47-3b6d-47ea-b694-19a4760bd635",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##### Ingest Batch Data and Capture Streaming Metrics of rExport to Azure Monitor Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e23312df-3b74-48a5-8811-808fa378926d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ingest batch data data into df\n",
    "def read_data_files(spark: SparkSession, file_path: str) -> DataFrame:\n",
    "    try:\n",
    "        pass\n",
    "        df = (\n",
    "            spark.read.format(\"json\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(file_path)\n",
    "            .withColumn(\n",
    "                \"tpep_dropoff_datetime\",\n",
    "                col=col(\"tpep_dropoff_datetime\").cast(TimestampType()),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"tpep_pickup_datetime\",\n",
    "                col=col(\"tpep_pickup_datetime\").cast(TimestampType()),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        properties = {\n",
    "            \"notebook_name\": \"observability_poc\",\n",
    "            \"taxidata_ingestion_status\": \"success\",\n",
    "            \"level\": \"info\",\n",
    "        }\n",
    "        logger.info(\n",
    "            \"Batch read of nytaxi json files, new column addition and data type convertion were successful.\",\n",
    "            extra=properties,\n",
    "            exc_info=True\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        properties = {\n",
    "            \"notebook_name\": \"observability_poc\",\n",
    "            \"read_data_files_error\": \"true\",\n",
    "        }\n",
    "        logger.error(\n",
    "            \"Exception occured\",\n",
    "            stack_info=True,\n",
    "            exc_info=True,\n",
    "            extra=properties,\n",
    "        )\n",
    "\n",
    "\n",
    "try:\n",
    "    pass\n",
    "    data_path = \"dbfs:/databricks-datasets/nyctaxi/sample/json/\"\n",
    "    df_json = read_data_files(spark=spark, file_path=data_path)\n",
    "    df_json.display()\n",
    "    record_count = df_json.count()\n",
    "    print(record_count)\n",
    "except Exception as e:\n",
    "    properties = {\n",
    "        \"notebook_name\": \"observability_poc\",\n",
    "        \"dataframe_display_error\": \"true\",\n",
    "    }\n",
    "    logger.error(\n",
    "        \"Error occurred reading and displaying dataframe\",\n",
    "        exc_info=True,\n",
    "        extra=properties,\n",
    "    )\n",
    "    print(\"check the azure monitor exception logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2d1d35-466d-47d9-b776-a7879c0aa18d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# datetime formats\n",
    "\n",
    "print(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S.%f\"))\n",
    "\n",
    "date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969b29aa-827c-4792-be12-d95250e632f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clear out data from previous demo execution\n",
    "\n",
    "username = spark.sql(\"SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')\").first()[0]\n",
    "checkpoint_path= \"abfss://adlscontainer@adls05.dfs.core.windows.net/jsondata/_checkpoint\"\n",
    "schema_location = \"abfss://adlscontainer@adls05.dfs.core.windows.net/jsondata/_schematracking\"\n",
    "target_path = \"abfss://adlscontainer@adls05.dfs.core.windows.net/jsondata/output\"\n",
    "table_name = f\"{username}_nytaxi\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(target_path, True)\n",
    "dbutils.fs.rm(schema_location, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a2310f-3257-4c02-800e-a49eb0dd471b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"cloudFiles.format\": \"json\",\n",
    "    \"cloudFiles.inferColumnTypes\": True,\n",
    "    \"cloudFiles.inferSchema\": True,\n",
    "    \"cloudFiles.schemaLocation\": schema_location,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "028ec587-1231-4aac-843e-c9b6a140d47e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Ingest Streaming Data and Capture Streaming Metrics of rExport to Azure Monitor Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1897b949-ecfc-44cf-a8d5-161b6c969a4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def start_streaming_query(target_path:str):\n",
    "    while True:\n",
    "        try:\n",
    "            q = (\n",
    "                spark.readStream.format(\"cloudFiles\")\n",
    "                .options(**options)\n",
    "                .load(\"dbfs:/databricks-datasets/nyctaxi/sample/json/\")\n",
    "                .writeStream.format(\"delta\")\n",
    "                .trigger(availableNow=True)\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .option(\"checkpointLocation\", checkpoint_path)\n",
    "                .queryName(\"nytaxi_query\")\n",
    "                .option(\"path\", target_path)\n",
    "                .start()\n",
    "            )\n",
    "            q.awaitTermination()\n",
    "            return q\n",
    "            properties = {\n",
    "            \"notebook_name\": \"observability_poc\",\n",
    "            \"notebook_cell_execution\": \"nytaxi data streaming query\",\n",
    "            }\n",
    "            logger.info(\"streaming query execution was successful\", exc_info=True, extra=properties)\n",
    "            print(\"check the azure monitor Apptraces logs\")\n",
    "        except BaseException as e:\n",
    "            properties = {\n",
    "            \"notebook_name\": \"observability_poc\",\n",
    "            \"notebook_cell_execution\": \"nytaxi data streaming query\",\n",
    "            }\n",
    "            logger.exception(\"streaming query execution encountered an exception\", exc_info=True, extra=properties)\n",
    "            print(\"check the azure monitor exception logs\")\n",
    "            # Adding a new column will trigger an UnknownFieldException. In this case we just restart the stream:\n",
    "            if not (\"UnknownFieldException\" in str(e.stackTrace)):\n",
    "                raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ccf0915-541e-421e-8470-f2fc98e5ea14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_ = start_streaming_query(target_path=target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95dfe3ff-b9b8-4ca0-bc61-f5aa1e258bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query_metrics = [{\n",
    "    \"TotalInputRowsCount\": query_.lastProgress[\"numInputRows\"],\n",
    "    \"ProcessedInputRowsPerSecond\": query_.lastProgress[\"processedRowsPerSecond\"],\n",
    "    \"InputSink\": query_.lastProgress[\"sink\"][\"description\"],\n",
    "    \"InputSources\": query_.lastProgress[\"sources\"][0][\"description\"],\n",
    "    \"TimeGenerated\": query_.lastProgress[\"timestamp\"],\n",
    "    \"Application\": \"observability_streaming_query\",\n",
    "    \"RequiredClusterfeature\": \"photon_predictive_io\",\n",
    "    \"JobId\": query_.lastProgress[\"id\"],\n",
    "    \"JobName\": query_.lastProgress[\"name\"],\n",
    "    \"CurrentTableName\": table_name\n",
    "}]\n",
    "\n",
    "query_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c58e766-b29b-4d34-9571-46564ef2acf4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Upload Streaming Metrics to Log Analytics Using the Logs Ingestion API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41cc235d-90db-4699-bbf6-89a7c2769f93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Log Ingestion Client for Azure Commercial Cloud\n",
    "# https://learn.microsoft.com/en-us/azure/azure-monitor/logs/notebooks-azure-monitor-logs\n",
    "\n",
    "dce_endpoint = \"https://logsingestionendpoint-jwl7.eastus-1.ingest.monitor.azure.com\" # ingestion endpoint of the Data Collection Endpoint object\n",
    "dcr_immutableid = \"dcr-bc9e64eba16345518a890476255b6827\" # immutableId property of the Data Collection Rule\n",
    "stream_name = \"Custom-ApacheSparkLogs_CL\" # name of the stream in the DCR that represents the destination table\n",
    "\n",
    "# credential = DefaultAzureCredential()\n",
    "client = LogsIngestionClient(endpoint=dce_endpoint, credential=credential, logging_enable=True)\n",
    "\n",
    "# configure the metrics publishing role on the DCR for this operation to be successful.\n",
    "try:\n",
    "    client.upload(rule_id=dcr_immutableid, stream_name=stream_name, logs=query_metrics)\n",
    "except HttpResponseError as e:\n",
    "    print(f\"Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4882c205-cd58-4a4b-8fca-4153066efacd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Information for configuring log ingestion client for non-public Azure cloud\n",
    "By default, LogsIngestionClient is configured to connect to the public Azure cloud. To connect to non-public Azure clouds, some additional configuration is required. The appropriate scope for authentication must be provided using the credential_scopes keyword argument. The following example shows how to configure the client to connect to Azure US Government:\n",
    "\n",
    "logs_client = LogsIngestionClient(endpoint, credential_scopes=[\"https://monitor.azure.us//.default\"])\n",
    "\n",
    "https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/monitor/azure-monitor-ingestion/README.md#configure-clients-for-non-public-azure-clouds"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "observability_nb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
