{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b4b8273-e01b-46a7-ac82-9734a91f3ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Import Required Libraries.\n",
    "Import the necessary libraries for Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3c9ad-328d-4e20-ab73-ec5650c8f03b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import DataFrame\n",
    "import azure.identity\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.ingestion import LogsIngestionClient\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from opentelemetry._logs import (\n",
    "    get_logger_provider,\n",
    "    set_logger_provider,\n",
    ")\n",
    "from opentelemetry.sdk._logs import (\n",
    "    LoggerProvider,\n",
    "    LoggingHandler,\n",
    ")\n",
    "from opentelemetry.sdk._logs.export import BatchLogRecordProcessor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorLogExporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f0b6122-d45f-4ab6-b6c8-cd26cf109992",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Set Up Application Insights Logger.\n",
    "Configure the logger to send metrics to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8017ce15-04ae-44c0-b002-d63553157bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "set_logger_provider(LoggerProvider())\n",
    "exporter = AzureMonitorLogExporter.from_connection_string(\n",
    "    dbutils.secrets.get(\"myscope\", key=\"appinsightsconnstr\")\n",
    ")\n",
    "get_logger_provider().add_log_record_processor(BatchLogRecordProcessor(exporter))\n",
    "\n",
    "# Attach LoggingHandler to namespaced logger\n",
    "handler = LoggingHandler()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cc1888b-9ffe-4892-9c34-fc635c2b4480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Step 4: Modify the StreamingQueryListener to Send Metrics.\n",
    "Update the ValueTrackingListener class to send metrics to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4e23a6-9625-45ae-b1d2-8f516df76ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "class ValueTrackingListener(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        logger.info(f\"'{event.name}' [{event.id}] got started!\")\n",
    "    \n",
    "    def onQueryProgress(self, event):\n",
    "        row = event.progress.observedMetrics.get(\"metric\")\n",
    "        if row is not None:\n",
    "            avg_value = row.avg_value\n",
    "            logger.info(f\"Recorded metric avg_value: {avg_value}\")\n",
    "            # Send custom metric to Application Insights\n",
    "            logger.info({\n",
    "                'custom_dimensions': {\n",
    "                    'avg_value': avg_value,\n",
    "                    'query_name': event.name,\n",
    "                    'query_id': event.id\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    def onQueryTerminated(self, event):\n",
    "        logger.info(f\"{event.id} got terminated!\")\n",
    "\n",
    "# Add listener\n",
    "listener = ValueTrackingListener()\n",
    "spark.streams.addListener(listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb895318-fde2-410e-84ab-8751e00e9eea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Step 5: Run the Streaming Query.\n",
    "Ensure your streaming query is set up to use the listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f636d9bb-48de-4136-9975-fa68a219fd16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "streaming_df = (spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 100)\n",
    "    .load())\n",
    "\n",
    "observed_streaming_df = streaming_df.observe(\n",
    "    \"metric\",\n",
    "    count(lit(1)).alias(\"cnt\"),  # number of processed rows\n",
    "    avg(col(\"value\")).alias(\"avg_value\"))  # average of row values\n",
    "\n",
    "# COMMAND ----------\n",
    "query = (observed_streaming_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .queryName(\"Rate query\")\n",
    "    .start())\n",
    "\n",
    "# COMMAND ----------\n",
    "import time\n",
    "time.sleep(120)\n",
    "query.stop()\n",
    "spark.streams.removeListener(listener)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sample-listerner",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
