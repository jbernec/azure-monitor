{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082ec8ed-5b7a-4584-8ab0-c1767e9b4258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### This setup will capture the streaming metrics and send them to Azure Application Insights, using the app insights connection string for autentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4b8273-e01b-46a7-ac82-9734a91f3ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 1: Import Required Libraries.\n",
    "Import the necessary libraries for Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d126d912-e136-45e6-a3e2-b1e85df3b422",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/Azure-Samples/databricks-observability/blob/main/modules/databricks/notebooks/telemetry-helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f3c9ad-328d-4e20-ab73-ec5650c8f03b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import DataFrame\n",
    "import azure.identity\n",
    "from azure.identity import DefaultAzureCredential, EnvironmentCredential, ManagedIdentityCredential, SharedTokenCacheCredential\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.ingestion import LogsIngestionClient\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from opentelemetry._logs import (\n",
    "    get_logger_provider,\n",
    "    set_logger_provider,\n",
    ")\n",
    "from opentelemetry.sdk._logs import (\n",
    "    LoggerProvider,\n",
    "    LoggingHandler,\n",
    ")\n",
    "from opentelemetry.sdk._logs.export import BatchLogRecordProcessor\n",
    "from azure.monitor.opentelemetry.exporter import AzureMonitorLogExporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f0b6122-d45f-4ab6-b6c8-cd26cf109992",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Step 3: Set Up Application Insights Logger.\n",
    "Configure the logger to send metrics to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8017ce15-04ae-44c0-b002-d63553157bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "set_logger_provider(LoggerProvider())\n",
    "exporter = AzureMonitorLogExporter.from_connection_string(\n",
    "    dbutils.secrets.get(\"myscope\", key=\"appinsightsconnstr\")\n",
    ")\n",
    "get_logger_provider().add_log_record_processor(BatchLogRecordProcessor(exporter))\n",
    "\n",
    "# Attach LoggingHandler to namespaced logger\n",
    "handler = LoggingHandler()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc1888b-9ffe-4892-9c34-fc635c2b4480",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Step 4: Modify the StreamingQueryListener to Send Metrics.\n",
    "Update the ValueTrackingListener class to send metrics to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4e23a6-9625-45ae-b1d2-8f516df76ac1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "class ValueTrackingListener(StreamingQueryListener):\n",
    "    def __init__(self):\n",
    "        self.total_rows = 0\n",
    "        self.total_avg_value = 0.0\n",
    "        self.num_batches = 0\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def onQueryStarted(self, event):\n",
    "        self.start_time = time.time()\n",
    "        logger.info(f\"'{event.name}' [{event.id}] got started!\")\n",
    "    \n",
    "    def onQueryProgress(self, event):\n",
    "        row = event.progress.observedMetrics.get(\"metric\")\n",
    "        if row is not None:\n",
    "            cnt = row.cnt\n",
    "            avg_value = row.avg_value\n",
    "            self.total_rows += cnt\n",
    "            self.total_avg_value += avg_value\n",
    "            self.num_batches += 1\n",
    "            logger.info(f\"Recorded metric avg_value: {avg_value}\")\n",
    "            # Send custom metric to Application Insights\n",
    "            logger.info({\n",
    "                'custom_dimensions': {\n",
    "                    'avg_value': avg_value,\n",
    "                    'query_name': event.name,\n",
    "                    'query_id': event.id\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    def onQueryTerminated(self, event):\n",
    "        self.end_time = time.time()\n",
    "        total_time_taken = self.end_time - self.start_time\n",
    "        if self.num_batches > 0:\n",
    "            overall_avg_value = self.total_avg_value / self.num_batches\n",
    "        else:\n",
    "            overall_avg_value = 0.0\n",
    "        logger.info(f\"{event.id} got terminated!\")\n",
    "        logger.info(f\"Total rows processed: {self.total_rows}\")\n",
    "        logger.info(f\"Overall average value: {overall_avg_value}\")\n",
    "        logger.info(f\"Number of batches: {self.num_batches}\")\n",
    "        logger.info(f\"Total time taken: {total_time_taken} seconds\")\n",
    "        \n",
    "        # Send total rows, number of batches, and total time taken to Application Insights\n",
    "        logger.info({\n",
    "            'custom_dimensions': {\n",
    "                'total_rows': self.total_rows,\n",
    "                'num_batches': self.num_batches,\n",
    "                'total_time_taken': total_time_taken,\n",
    "                'query_name': event.name,\n",
    "                'query_id': event.id\n",
    "            }\n",
    "        })\n",
    "\n",
    "# Add listener\n",
    "listener = ValueTrackingListener()\n",
    "spark.streams.addListener(listener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb895318-fde2-410e-84ab-8751e00e9eea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Step 5: Run the Streaming Query.\n",
    "Ensure your streaming query is set up to use the listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f636d9bb-48de-4136-9975-fa68a219fd16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "streaming_df = (spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 10)\n",
    "    .load())\n",
    "\n",
    "observed_streaming_df = streaming_df.observe(\n",
    "    \"metric\",\n",
    "    count(lit(1)).alias(\"cnt\"),  # number of processed rows\n",
    "    avg(col(\"value\")).alias(\"avg_value\"))  # average of row values\n",
    "\n",
    "# COMMAND ----------\n",
    "query = (observed_streaming_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .queryName(\"Rate query\")\n",
    "    .start())\n",
    "\n",
    "# COMMAND ----------\n",
    "import time\n",
    "time.sleep(120)\n",
    "query.stop()\n",
    "spark.streams.removeListener(listener)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sample-listernerv2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
